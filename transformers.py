# -*- coding: utf-8 -*-
"""transformers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y5PJ-HO875SPEKVGScgmp-WDUXiao_tR
"""

!pip install transformers

from transformers import pipeline

model = pipeline("summarization", "sshleifer/distilbart-cnn-12-6")

text = '''
BERT (Bidirectional Encoder Representations from Transformers) is a recent paper published by researchers at Google AI Language.
It has caused a stir in the Machine Learning community by presenting state-of-the-art results in a wide variety of NLP tasks, including Question Answering (SQuAD v1.1), Natural Language Inference (MNLI), and others.

BERT’s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling.
This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training.
The paper’s results show that a language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models. In the paper, the researchers detail a novel technique named Masked LM (MLM) which allows bidirectional training in models in which it was previously impossible.
'''

ans = model(text, max_new_tokens=100)

ans

result = ans[0]['summary_text'].split(".")

for i in result:
  if len(i.split()) > 5:
    print(i+".")

# read a document and make a summary of 50 words. and give same summary document.

# 3 application of generative AI (Question/ Answering)

model = pipeline("question-answering")

context = '''
A computer is an electronic machine that is used to store, process, and manage data. It works according to the instructions given by the user. Computers help us perform calculations, create documents, browse the internet, and communicate with others. They are widely used in education, business, healthcare, and entertainment. With the help of computers, work becomes faster, easier, and more accurate.
'''

result = model(question="what is computer", context=context)

result

while True:
  text = input("question: ")
  if text == "exit":
    break
  if text.split(":")[0] == "context":
    context = text
    print("\nresponse: ", "context change to -\n", text, "\n\n")
    continue
  result = model(question=text, context=context)
  print("answer: ", result['answer'])

# 4. application of generative AI (translation (language translation))  work on parellel corpus

translator = pipeline("translation_en_to_fr")

p = "whats your name"
result = translator(p)

result

translator2 = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")

eng = translator2(result[0]['translation_text'])

eng

"""PDF text summarization into a single page PDF"""

!pip install PyPDF2

from PyPDF2 import PdfReader

reader = PdfReader("Generative_AI_and_LLMs_Full.pdf")

text_data = []
for page in reader.pages:
    text_data.append(page.extract_text())

print(text_data)

summary = pipeline("summarization")

summary_result = []

for i in text_data:
  print(i)
  ans = summary(i, max_new_tokens=100)
  summary_result.append(ans)

summary_result

!pip install fpdf

pdf_text = ""
for i in summary_result:
  pdf_text += i[0]['summary_text'] + "\n"

pdf_text

from fpdf import FPDF

pdf = FPDF()
pdf.add_page()
pdf.set_font("Arial", size=12)

pdf.multi_cell(0, 8, pdf_text)

pdf.output("output.pdf")

